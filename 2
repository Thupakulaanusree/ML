import pandas as pd
import numpy as np
import math

# Load the dataset (make sure to exclude 'Day' column from features)
df = pd.read_csv('/content/enjoysport_dataset.csv')

# Remove the 'Day' column as it is not relevant to decision making
df = df.drop('Day', axis=1)

# Assuming the last column is the target column (play or not play)
target = df.columns[-1]

# Function to calculate entropy
def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy = 0
    for i in range(len(elements)):
        probability = counts[i] / sum(counts)
        entropy -= probability * math.log2(probability)
    return entropy

# Function to calculate information gain
def information_gain(data, split_attribute, target_attribute):
    total_entropy = entropy(data[target_attribute])
    values, counts = np.unique(data[split_attribute], return_counts=True)
    
    weighted_entropy = 0
    for i in range(len(values)):
        subset = data[data[split_attribute] == values[i]]
        weight = counts[i] / sum(counts)
        weighted_entropy += weight * entropy(subset[target_attribute])
    
    info_gain = total_entropy - weighted_entropy
    return info_gain

# Function to build the decision tree using ID3
def decision_tree(data, original_data, features, target_attribute, parent_node_class=None):
    # If all target values have the same class, return that class
    if len(np.unique(data[target_attribute])) <= 1:
        return np.unique(data[target_attribute])[0]
    
    # If the dataset is empty or no features are left, return the most common class of the original dataset
    elif len(data) == 0 or len(features) == 0:
        return np.unique(original_data[target_attribute])[np.argmax(np.unique(original_data[target_attribute], return_counts=True)[1])]
    
    # If the dataset is not empty, proceed to build the tree
    else:
        # Select the best feature based on Information Gain
        best_feature = max(features, key=lambda feature: information_gain(data, feature, target_attribute))
        
        # Create the root node for the decision tree
        tree = {best_feature: {}}
        
        # Remove the selected feature from the feature set
        features = [i for i in features if i != best_feature]
        
        # Grow a branch under the root node for each value of the best feature
        for value in np.unique(data[best_feature]):
            subset = data[data[best_feature] == value]
            subtree = decision_tree(subset, original_data, features, target_attribute)
            tree[best_feature][value] = subtree
        
        return tree

# Function to visualize the tree in the format you desire
def print_tree(tree, indent=""):
    # If it's a leaf node (i.e., final decision)
    if not isinstance(tree, dict):
        print(indent + "Decision: " + str(tree))
    else:
        # The tree is a dictionary, so get the root node (first key)
        for key, value in tree.items():
            for subkey, subtree in value.items():
                # Print the current attribute and value, then recurse
                print(indent + str(key) + " -> " + str(subkey))
                print_tree(subtree, indent + "|   ")

# Get the feature names (all columns except the last one, which is the target)
features = list(df.columns[:-1])

# Build the tree
tree = decision_tree(df, df, features, target)

# Print the tree in the hierarchical format
print("Final ID3 Decision Tree:")
print_tree(tree)
